{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libary"
      ],
      "metadata": {
        "id": "XMDSVF829PBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74vz3tvK87_K"
      },
      "outputs": [],
      "source": [
        "# Install libary necessary\n",
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1.0 --progress-bar off\n",
        "!pip install -qqq transformers==4.34.0 --progress-bar off\n",
        "!pip install -qqq langchain --progress-bar off\n",
        "!pip install -qqq chromadb --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq InstructorEmbedding==1.0.1 --progress-bar off\n",
        "!pip install -qqq websocket-client --progress-bar off\n",
        "!pip install -qqq gradio --progress-bar off\n",
        "!pip install -qqq python-engineio==3.14.2 python-socketio==4.6.0 --progress-bar off\n",
        "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl\n",
        "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from langchain import HuggingFacePipeline, PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "import locale\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "iwQ_49tr9SWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ehyzr7tX9UGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "- Data gathering and preprocessing\n"
      ],
      "metadata": {
        "id": "xvqvXuz_9WQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/demo_websocket/list_stock.json', 'r') as f:\n",
        "      data_stocks = json.load(f)\n",
        "symbols = []  # Add more symbols if needed\n",
        "\n",
        "for i in range(0,len(data_stocks['results'])):\n",
        "  symbols.append(data_stocks['results'][i]['symbol'])"
      ],
      "metadata": {
        "id": "J5D0KRuM9UjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exchange_slugs = []  # Add more exchanges if needed\n",
        "symbols = []  # Add more symbols if needed\n",
        "\n",
        "for i in range(0,len(data_stocks['results'])):\n",
        "  symbols.append(data_stocks['results'][i]['symbol'])\n",
        "  exchange_slugs.append(data_stocks['results'][i]['exchange_slug'])"
      ],
      "metadata": {
        "id": "XQqDrZyX9ZUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def financial_data(url, exchange_slug, symbol):\n",
        "    data = {\n",
        "        'exchange_slug': exchange_slug,\n",
        "        'symbol': symbol\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            result = response.json()\n",
        "            return result\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON response from URL {url}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Yêu cầu không thành công cho URL {url}. Mã trạng thái: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "urls = [\n",
        "    your url US Stock API",
        "]\n",
        "\n",
        "\n",
        "\n",
        "result_filenames = {\n",
        "    urls[0]: \"Revenue.txt\",\n",
        "    urls[1]: \"Net-Income.txt\",\n",
        "    urls[2]: \"Cash-Flow.txt\",\n",
        "    urls[3]: \"EPS.txt\",\n",
        "    urls[4]: \"DTER.txt\"\n",
        "}\n",
        "financials_labels = [\"Revenue\", \"Net Income\", \"Cash Flow\", \"EPS\", \"DTER\"]"
      ],
      "metadata": {
        "id": "2GLe4DvL9ars"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "for url in urls:\n",
        "    for i in range(len(symbols)):\n",
        "        result = financial_data(url, exchange_slugs[i], symbols[i])\n",
        "        if result is not None:\n",
        "            filename = result_filenames.get(url, \"Unknown.txt\")\n",
        "            label_index = urls.index(url)\n",
        "\n",
        "            with open(filename, 'a') as txt_file:\n",
        "                for index, entry in enumerate(result['data']):\n",
        "                    value = entry['value']\n",
        "                    time = entry['time']\n",
        "                    dt = datetime.now()\n",
        "                    txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} on {time}\\n\")\n",
        "                    txt_file.write(f\"On {time},{financials_labels[label_index]} of {symbols[i]} stock is {value}\\n\")\n",
        "\n",
        "                    year = (datetime.strptime(time, \"%Y-%m-%d\")).year\n",
        "                    month = str(math.ceil(int((datetime.strptime(time, \"%Y-%m-%d\")).month)/3))\n",
        "                    month_recent=str(math.floor(dt.month/3))\n",
        "                    if month_recent=='0':\n",
        "                       month_recent='4'\n",
        "                    txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the Q{month} of {year}\\n\")\n",
        "                    txt_file.write(f\"In the Q{month} of {year},{financials_labels[label_index]} of {symbols[i]} stock is {value}\\n\")\n",
        "\n",
        "                    if f'Q{month}' == \"Q1\":\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the first quarter of {year}\\n\")\n",
        "                    elif f'Q{month}' == \"Q2\":\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the second quarter of {year}\\n\")\n",
        "                    elif f'Q{month}' == \"Q3\":\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the third quarter of {year}\\n\")\n",
        "                    elif f'Q{month}' == \"Q4\":\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the fourth quarter of {year}\\n\")\n",
        "                    if month_recent == month and dt.year == year:\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock is {value} in the last quarter of {year}\\n\")\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock now is {value}\\n\")\n",
        "                      txt_file.write(f\"{financials_labels[label_index]} of {symbols[i]} stock recent is {value}\\n\")"
      ],
      "metadata": {
        "id": "jbdJHIlj9cQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a list of filenames to merge\n",
        "filenames_to_merge = [\"Revenue.txt\", \"Net-Income.txt\", \"Cash-Flow.txt\", \"EPS.txt\", \"DTER.txt\"]\n",
        "\n",
        "# Name of the output merged file\n",
        "output_filename = \"/content/drive/MyDrive/data/financial_data.txt\"\n",
        "\n",
        "# Open the output file in write mode to overwrite existing data\n",
        "with open(output_filename, 'w') as output_file:\n",
        "    # Loop through each filename to merge\n",
        "    for filename in filenames_to_merge:\n",
        "        try:\n",
        "            with open(filename, 'r') as input_file:\n",
        "                # Read the content of the input file\n",
        "                content = input_file.read()\n",
        "                # Append the content to the output file\n",
        "                output_file.write(content)\n",
        "                # Add a separator between merged files\n",
        "                output_file.write(f\"--\\n\")\n",
        "            print(f\"File {filename} has been merged into {output_filename}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File {filename} not found, skipping.\")\n",
        "\n",
        "print(f\"All files have been merged into {output_filename} and existing data has been overwritten.\")\n"
      ],
      "metadata": {
        "id": "YLtUybxK9eX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "- load data\n",
        "- split text\n",
        "- model embedding\n",
        "- model LLM"
      ],
      "metadata": {
        "id": "cLR7QbDJ9jQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data from drive\n",
        "def load_documents():\n",
        "    loader = DirectoryLoader('/content/drive/MyDrive/Data', glob=\"*.txt\", loader_cls=TextLoader)\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "\n",
        "def split_text_into_chunks(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=70, chunk_overlap=0)\n",
        "    text_chunks = text_splitter.split_documents(documents)\n",
        "    return text_chunks\n",
        "\n",
        "# Load model embeddings ember-v1\n",
        "def create_embeddings():\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "      model_name=\"llmrails/ember-v1\", model_kwargs={\"device\": DEVICE}\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "# Load Chroma DB and store vector embeddings\n",
        "def create_vector_store(text_chunks, embeddings):\n",
        "    db = Chroma.from_documents(text_chunks, embeddings, persist_directory=\"db\")\n",
        "    return db\n",
        "\n",
        "# Load model LLMs\n",
        "def create_llms_model():\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "    model = AutoGPTQForCausalLM.from_quantized(\n",
        "        model_name_or_path,\n",
        "        revision=\"gptq-4bit-32g-actorder_True\",\n",
        "        # model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        inject_fused_attention=False,\n",
        "        device=DEVICE,\n",
        "        quantize_config=None,\n",
        "    )\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "d6bjIZV69jrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = create_embeddings()"
      ],
      "metadata": {
        "id": "Wv3z7Xn29ly9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = create_llms_model()"
      ],
      "metadata": {
        "id": "ts143yPq9ndU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define System Prompt"
      ],
      "metadata": {
        "id": "6XSrWj-B9rTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format output use Prompt\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "{prompt} [/INST]\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "WvLI8WRe9o88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
        "\n",
        "template = generate_prompt(\n",
        "    \"\"\"\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\",\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        ")"
      ],
      "metadata": {
        "id": "jdJfvJdE9uI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "ett9dnqf9vaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipline text-generation\n",
        "## Edit output based on custom parameters\n",
        "### Custom output use parameters:\n",
        " - max_new_tokens: tùy chỉnh ngữ cảnh đầu ra, nếu vượt quá, câu trả lời có thể bị cắt ngắn.\n",
        " - top_p: chi phối việc lựa chọn mã thông báo dựa trên xác suất.\n",
        " - temperature: xác định mức độ sáng tạo của câu trả lời. Giá trị thấp (gần bằng 0) tạo ra câu trả lời cụ thể hơn, trong khi giá trị cao (ví dụ: 1) tạo ra câu trả lời sáng tạo hơn.\n",
        " - repetition_penalty: chi phối việc lặp lại mã thông báo trong câu trả lời.\n"
      ],
      "metadata": {
        "id": "5q4wHj_893iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=3096,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=0.9,\n",
        "    streamer=streamer,\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0.01})"
      ],
      "metadata": {
        "id": "iQ0dBUju92P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you run it for the first time, run these blocks to load your data and embed it into the database, then you can use that database as a base knowledge."
      ],
      "metadata": {
        "id": "4y72eQ6HCpin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docs = load_documents()\n",
        "# text_chunks = split_text_into_chunks(docs)\n",
        "# db = create_vector_store(text_chunks, embeddings)\n",
        "# qa_chain = RetrievalQA.from_chain_type(\n",
        "#   llm=llm,\n",
        "#   chain_type=\"stuff\",\n",
        "#   retriever=db.as_retriever(search_kwargs={\"k\": 1}),\n",
        "#   return_source_documents=True,\n",
        "#   chain_type_kwargs={\"prompt\": prompt},\n",
        "#   )"
      ],
      "metadata": {
        "id": "y7O7d4hS991k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db.similarity_search(\"Current price of RHHBY\", k=5)\n",
        "# Results=qa_chain(\"Current price of RHHBY?\")['result']\n"
      ],
      "metadata": {
        "id": "rbtOUsUe9_Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load ChormaDB\n",
        "### load knowdegle base"
      ],
      "metadata": {
        "id": "voX77K4N-BlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/content\""
      ],
      "metadata": {
        "id": "VskdwBbp-BM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "UfnpRe_B-Ebz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/dbtrue.zip"
      ],
      "metadata": {
        "id": "7qzeX16V-Fez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db1 = Chroma(persist_directory=\"/content/content/db\", embedding_function=embeddings)\n",
        "qa_chain1 = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db1.as_retriever(search_kwargs={\"k\": 2}),\n",
        "  return_source_documents=True,\n",
        "  chain_type_kwargs={\"prompt\": prompt},\n",
        "  )"
      ],
      "metadata": {
        "id": "MD9cw0db-HFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db1.similarity_search(\"Net income of AAPL stock last quarter\", k=5)"
      ],
      "metadata": {
        "id": "sW034Yxt-J7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db1.get(where={'source':'/content/drive/MyDrive/Data/financial_data (1).txt'})"
      ],
      "metadata": {
        "id": "Zw279Qzp-M6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Results=qa_chain1(\"Revenue of AAPL in Q3 of 2022\")['result']"
      ],
      "metadata": {
        "id": "9_1wfMb0-ONW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/drive/MyDrive/dbtrue.zip /content/db"
      ],
      "metadata": {
        "id": "TyIsr808-PRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with Multiple .txt file using gradio\n",
        "\n"
      ],
      "metadata": {
        "id": "FYFAWGqi-Qx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create interface use Gradio\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def is_greeting(message):\n",
        "    greetings = [\"hello\", \"hi\", \"hey\", \"greetings\"]\n",
        "    return any(greeting in message.lower() for greeting in greetings)\n",
        "\n",
        "def predict(message, history):\n",
        "    if is_greeting(message):\n",
        "        return \"Hello! I'm your StockScan.io ChatBot. How can I help you with stock-related queries today?\"\n",
        "    else:\n",
        "        return qa_chain1(message)['result']\n",
        "\n",
        "css_code_light = \"\"\"\n",
        "    .gradio-container {\n",
        "        background-color: white;\n",
        "        color: green;\n",
        "    }\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    fn=predict,\n",
        "    css=css_code_light,\n",
        "    title='ChatBot US_Stock StockScan.io',\n",
        "    theme=theme,\n",
        "    input_size=(400, 50)\n",
        "    examples=[\n",
        "        [\"Revenue of AAPL in Q3 of 2022\"],\n",
        "        [\"In the Q1 of 2022, Revenue of AAPL stock\"],\n",
        "        [\"Revenue of AAPL stock in the last quarter\"],\n",
        "        [\"Revenue of AAPL stock now\"],\n",
        "        [\"Current price of TESLA\"],\n",
        "    ],\n",
        "    )\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "wvX7r3te-RfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update data real time and run gradio\n",
        "\n"
      ],
      "metadata": {
        "id": "C6EEtwMf-U7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/demo_websocket/list_stock.json', 'r') as f:\n",
        "      data_stocks = json.load(f)\n",
        "symbols = []  # Add more symbols if needed\n",
        "\n",
        "for i in range(0,len(data_stocks['results'])):\n",
        "  symbols.append(data_stocks['results'][i]['symbol'])"
      ],
      "metadata": {
        "id": "EWUkgVmd-TES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import socketio\n",
        "import json\n",
        "import threading\n",
        "import gradio as gr\n",
        "# Init Socket.IO client\n",
        "sio = socketio.Client(logger=True, engineio_logger=True)\n",
        "\n",
        "# list stock\n",
        "listcoins = symbols[:50]\n",
        "\n",
        "# price of stock\n",
        "coin_prices = {}\n",
        "\n",
        "# Lock to ensure thread-safe access to shared data\n",
        "lock = threading.Lock()\n",
        "\n",
        "@sio.on('connect')\n",
        "def on_connect():\n",
        "    print(\"Connected\")\n",
        "    sio.emit(\"RealTimeAvgPriceSubAdd\", {\n",
        "        'subs': listcoins\n",
        "    })\n",
        "\n",
        "@sio.on('avg_price_update')\n",
        "def handle_global_price_update(data):\n",
        "   with lock:\n",
        "        try:\n",
        "            stock = data['symbol']\n",
        "            price = data['price']\n",
        "            coin_prices[stock] = price\n",
        "\n",
        "            # if all(symbol in coin_prices for symbol in listcoins):\n",
        "            with open('/content/drive/MyDrive/data/prices.txt', 'w') as txt_file:\n",
        "              for symbol, price in coin_prices.items():\n",
        "                txt_file.write(f\"Current price of {symbol} stock is {price}$\\n\")\n",
        "        except KeyError as e:\n",
        "            print(f\"Error: Missing key {e} in data.\")\n",
        "\n",
        "@sio.on('disconnect')\n",
        "def disconnect():\n",
        "    print('Disconnected')\n",
        "sio.connect(url='your web socket url', transports=['websocket'])\n",
        "def is_greeting(message):\n",
        "    greetings = [\"hello\", \"hi\", \"hey\", \"greetings\"]\n",
        "    return any(greeting in message.lower() for greeting in greetings)\n",
        "\n",
        "def predict(message, history):\n",
        "    if is_greeting(message):\n",
        "        return \"Hello! I'm your StockScan.io ChatBot. How can I help you with stock-related queries today?\"\n",
        "    else:\n",
        "        return qa_chain1(message)['result']\n",
        "demo = gr.ChatInterface(\n",
        "    fn=predict,\n",
        "    title='ChatBot US_Stock StockScan.io',\n",
        "    examples=[\n",
        "        [\"Revenue of AAPL in Q3 of 2022\"],\n",
        "        [\"In the Q1 of 2022, Revenue of AAPL stock\"],\n",
        "        [\"Revenue of AAPL stock in the last quarter\"],\n",
        "        [\"Revenue of AAPL stock now\"],\n",
        "        [\"Current price of TSLA\"],\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n",
        "while(True):\n",
        "    loader = DirectoryLoader('/content/drive/MyDrive/data', glob=\"*.txt\", loader_cls=TextLoader)\n",
        "    documents1 = loader.load()\n",
        "    text_chunks1 = split_text_into_chunks(documents1)\n",
        "    list_id=db1.get(offset=906227,limit=50)['ids']\n",
        "    if list_id:\n",
        "      for i in range(len(text_chunks1)):\n",
        "          db1.update_document(document_id=list_id[i], document=text_chunks1[i])\n",
        "\n",
        "sio.wait()\n"
      ],
      "metadata": {
        "id": "2Bduxo9F-YCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
